# A project to describe the Extract, Trafnsform and Load (ETL) Approached using python. 

Data Pipelines are used by the companies in order to take the raw data (Extraction, mostly from the server log files), transfroming these data to the appropriate format (Transformation), and finally edit and push them to one or more databases. This process is called ETL, and in this repository we want to cover the most related approach in this area and also cover at least one more diverse project. 

Another project covered in this repository is a sample data pipeline designed to simulate the one job of the google analytics.
In this project we go through the process of ETL through a data pipeline and implement these steps:
- 1- producing the log data 
- 2- storing the extected data on the files and database
- 3- loading the data and the driven results 
